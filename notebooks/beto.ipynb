{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importaci√≥n de Librer√≠as\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import json  # <-- Importamos la librer√≠a para trabajar con JSON\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Declaraci√≥n de Funciones\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, label_encoder, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1)\n",
    "    return label_encoder.inverse_transform(predicted_class.cpu())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ab164801f645e2ac6fab229e944772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0b3192551248b5865dd1709206cfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b683021303402984a1b79bf5c7f2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68f8c65d32247c1b273cb38da42e2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41002249d1964cdda93a80a166cbdba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Inicializaci√≥n del Modelo\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ignacio-ave/BETO-nlp-sentiment-analysis-spanish\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"ignacio-ave/BETO-nlp-sentiment-analysis-spanish\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "label_encoder = LabelEncoder().fit(['P+', 'P', 'NEU', 'NONE', 'N', 'N+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Tweet\": \"Don Juan Fuentes ayer sali√≥ de paseo por las nuevas estaciones de #L3. ‚ÄúEl Metro es lo m√°s bac√°n que ha pasado en Quilicura‚Äù ü§é Gracias don Juan, para nosotros es un honor üöá\",\n",
      "    \"Predicci√≥n\": \"P+\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_tweet = \"Don Juan Fuentes ayer sali√≥ de paseo por las nuevas estaciones de #L3. ‚ÄúEl Metro es lo m√°s bac√°n que ha pasado en Quilicura‚Äù ü§é Gracias don Juan, para nosotros es un honor üöá\"\n",
    "\n",
    "prediction = predict_sentiment(my_tweet, model, tokenizer, label_encoder, device)\n",
    "result_individual = {\n",
    "    \"Tweet\": my_tweet,\n",
    "    \"Predicci√≥n\": prediction\n",
    "}\n",
    "\n",
    "print(json.dumps(result_individual, ensure_ascii=False, indent=4))  # <-- Imprimimos en formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple\n",
    "\n",
    "tweets = [\n",
    "    \"Me encanta usar este modelo para an√°lisis de sentimiento.\",\n",
    "    \"Este producto no cumpli√≥ mis expectativas.\",\n",
    "    \"El clima est√° bastante agradable hoy.\",\n",
    "    \"Me siento devastado por las noticias recientes.\",\n",
    "    \"La pel√≠cula estuvo regular, no fue ni buena ni mala.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Tweet\": \"Me encanta usar este modelo para an√°lisis de sentimiento.\",\n",
      "        \"Predicci√≥n\": \"P+\"\n",
      "    },\n",
      "    {\n",
      "        \"Tweet\": \"Este producto no cumpli√≥ mis expectativas.\",\n",
      "        \"Predicci√≥n\": \"N\"\n",
      "    },\n",
      "    {\n",
      "        \"Tweet\": \"El clima est√° bastante agradable hoy.\",\n",
      "        \"Predicci√≥n\": \"P+\"\n",
      "    },\n",
      "    {\n",
      "        \"Tweet\": \"Me siento devastado por las noticias recientes.\",\n",
      "        \"Predicci√≥n\": \"N+\"\n",
      "    },\n",
      "    {\n",
      "        \"Tweet\": \"La pel√≠cula estuvo regular, no fue ni buena ni mala.\",\n",
      "        \"Predicci√≥n\": \"P\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "results_multiple = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    prediction = predict_sentiment(tweet, model, tokenizer, label_encoder, device)\n",
    "    results_multiple.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Predicci√≥n\": prediction\n",
    "    })\n",
    "\n",
    "print(json.dumps(results_multiple, ensure_ascii=False, indent=4))  # <-- Imprimimos en formato JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_sentiment(text, model, tokenizer, label_encoder, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Convertir logits a probabilidades usando softmax\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    return probabilities[0].cpu().numpy()\n",
    "\n",
    "labels = ['P+', 'P', 'NEU', 'NONE', 'N', 'N+']\n",
    "label_encoder = LabelEncoder().fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Don Juan Fuentes ayer sali√≥ de paseo por las nuevas estaciones de #L3. ‚ÄúEl Metro es lo m√°s bac√°n que ha pasado en Quilicura‚Äù ü§é Gracias don Juan, para nosotros es un honor üöá\n",
      "---\n",
      "P+: 0.000\n",
      "P: 0.001\n",
      "NEU: 0.001\n",
      "NONE: 0.000\n",
      "N: 0.005\n",
      "N+: 0.992\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Realizar una Predicci√≥n\n",
    "\n",
    "## Individual\n",
    "\n",
    "my_tweet = \"Don Juan Fuentes ayer sali√≥ de paseo por las nuevas estaciones de #L3. ‚ÄúEl Metro es lo m√°s bac√°n que ha pasado en Quilicura‚Äù ü§é Gracias don Juan, para nosotros es un honor üöá\"\n",
    "probabilities = predict_sentiment(my_tweet, model, tokenizer, label_encoder, device)\n",
    "\n",
    "print(f\"Tweet: {my_tweet}\")\n",
    "print(\"---\")\n",
    "for label, probability in zip(labels, probabilities):\n",
    "    print(f\"{label}: {probability:.3f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Me encanta usar este modelo para an√°lisis de sentimiento.\n",
      "---\n",
      "P+: 0.000\n",
      "P: 0.001\n",
      "NEU: 0.001\n",
      "NONE: 0.001\n",
      "N: 0.021\n",
      "N+: 0.976\n",
      "\n",
      "\n",
      "Tweet: Este producto no cumpli√≥ mis expectativas.\n",
      "---\n",
      "P+: 0.906\n",
      "P: 0.062\n",
      "NEU: 0.025\n",
      "NONE: 0.003\n",
      "N: 0.002\n",
      "N+: 0.001\n",
      "\n",
      "\n",
      "Tweet: El clima est√° bastante agradable hoy.\n",
      "---\n",
      "P+: 0.000\n",
      "P: 0.001\n",
      "NEU: 0.001\n",
      "NONE: 0.001\n",
      "N: 0.013\n",
      "N+: 0.985\n",
      "\n",
      "\n",
      "Tweet: Me siento devastado por las noticias recientes.\n",
      "---\n",
      "P+: 0.471\n",
      "P: 0.512\n",
      "NEU: 0.013\n",
      "NONE: 0.001\n",
      "N: 0.001\n",
      "N+: 0.001\n",
      "\n",
      "\n",
      "Tweet: La pel√≠cula estuvo regular, no fue ni buena ni mala.\n",
      "---\n",
      "P+: 0.002\n",
      "P: 0.001\n",
      "NEU: 0.024\n",
      "NONE: 0.001\n",
      "N: 0.613\n",
      "N+: 0.360\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Multiple\n",
    "\n",
    "tweets = [\n",
    "    \"Me encanta usar este modelo para an√°lisis de sentimiento.\",\n",
    "    \"Este producto no cumpli√≥ mis expectativas.\",\n",
    "    \"El clima est√° bastante agradable hoy.\",\n",
    "    \"Me siento devastado por las noticias recientes.\",\n",
    "    \"La pel√≠cula estuvo regular, no fue ni buena ni mala.\"\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    probabilities = predict_sentiment(tweet, model, tokenizer, label_encoder, device)\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(\"---\")\n",
    "    for label, probability in zip(labels, probabilities):\n",
    "        print(f\"{label}: {probability:.3f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
